{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Author: James Goppert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writeup\n",
    "\n",
    "\n",
    "## Provide a Writeup / README that includes all the rubric points and how you addressed each one. You can submit your writeup as markdown or pdf.?\n",
    "\n",
    " * The writeup / README should include a statement and supporting figures / images that explain how each rubric item was addressed, and specifically where in the code each step was handled.\n",
    " \n",
    " **YOU ARE READING IT!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Run the functions provided in the notebook on test images (first with the test data provided, next on data you have recorded). Add/modify functions to allow for color selection of obstacles and rock samples.\n",
    "\n",
    " * Describe in your writeup (and identify where in your code) how you modified or added functions to add obstacle and rock sample identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    " \n",
    "## Populate the process_image() function with the appropriate analysis steps to map pixels identifying navigable terrain, obstacles and rock samples into a worldmap. Run process_image() on your test data using the moviepy functions provided to create video output of your result.\n",
    "\n",
    " * Describe in your writeup how you modified the process_image() to demonstrate your analysis and how you created a worldmap. Include your video output with your submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomous Navigation and Mapping\n",
    "\n",
    "\n",
    "## Fill in the perception_step() (at the bottom of the perception.py script) and decision_step() (in decision.py) functions in the autonomous mapping scripts and an explanation is provided in the writeup of how and why these functions were modified as they were.\n",
    "\n",
    " * perception_step() and decision_step() functions have been filled in and their functionality explained in the writeup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching in autonomous mode your rover can navigate and map autonomously. Explain your results and how you might improve them in your writeup.\n",
    "\n",
    " * By running drive_rover.pyand launching the simulator in autonomous mode, your rover does a reasonably good job at mapping the environment.\n",
    "\n",
    " * The rover must map at least 40% of the environment with 60% fidelity (accuracy) against the ground truth. You must also find (map) the location of at least one rock sample. They don't need to pick any rocks up, just have them appear in the map (should happen automatically if their map pixels in Rover.worldmap[:,:,1] overlap with sample locations.)\n",
    "\n",
    " * Note: running the simulator with different choices of resolution and graphics quality may produce different results, particularly on different machines! Make a note of your simulator settings (resolution and graphics quality set on launch) and frames per second (FPS output to terminal by drive_rover.py) in your writeup when you submit the project so your reviewer can reproduce your results.\n",
    " \n",
    " ![graphics settings](calib_dataset/graphics_settings.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "\n",
    "* Jupyter Notebook with your test code [Jupyter Notebook](./code/Rover_Project_Test_Notebook.ipynb)\n",
    "* Test output video [Test Output Video](./output/test_mapping.mp4)\n",
    "* Autonomous navigation scripts\n",
    "  * [drive_rover.py](code/drive_rover.py)\n",
    "  * [supporting_functions.py](code/supporting_functions.py)\n",
    "  * [decision.py](code/decision.py)\n",
    "  * [perception.py](code/perception.py)\n",
    "* writeup report (md or pdf file)\n",
    "  * [pdf](roboND_jgoppert_p1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Project - Search and Sample Return.ipynb to PDF\n",
      "[NbConvertApp] Writing 21083 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 61001 bytes to roboND_jgoppert_p1.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert Project\\ -\\ Search\\ and\\ Sample\\ Return.ipynb --to PDF --output roboND_jgoppert_p1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

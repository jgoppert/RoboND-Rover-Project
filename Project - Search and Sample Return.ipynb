{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Author: James Goppert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writeup\n",
    "\n",
    "\n",
    "## Provide a Writeup / README that includes all the rubric points and how you addressed each one. You can submit your writeup as markdown or pdf.?\n",
    "\n",
    " * The writeup / README should include a statement and supporting figures / images that explain how each rubric item was addressed, and specifically where in the code each step was handled.\n",
    " \n",
    " **YOU ARE READING IT!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Run the functions provided in the notebook on test images (first with the test data provided, next on data you have recorded). Add/modify functions to allow for color selection of obstacles and rock samples.\n",
    "\n",
    " * Describe in your writeup (and identify where in your code) how you modified or added functions to add obstacle and rock sample identification.\n",
    " \n",
    "\n",
    "### Answer\n",
    "\n",
    " In order to do the color segmentation I used hue saturation and value ranges. The rocks were highly saturated, with low hue, and high value. The ground was higher intensity than the obstacles, but both had low saturation. I also used some open and close operations to  remove any dots.\n",
    " \n",
    " ![image](output/segmentation.png)\n",
    " \n",
    " ```python\n",
    " def color_segmenter(img):\n",
    "    \"\"\"\n",
    "    More advanced version of color_thresh\n",
    "    that does entire image segmentation\n",
    "\n",
    "    @param: img: the input image\n",
    "    @return: dict with images for each type of background\n",
    "        and a composite for visualization\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "\n",
    "    thresh_rock = cv2.inRange(\n",
    "        hsv,\n",
    "        np.array([0, 200, 100], dtype=np.float),\n",
    "        np.array([100, 255, 255], dtype=np.float))\n",
    "    thresh_rock = cv2.morphologyEx(thresh_rock, cv2.MORPH_CLOSE, kernel)\n",
    "    thresh_rock = thresh_rock > 0\n",
    "\n",
    "    thresh_ground = cv2.inRange(hsv,\n",
    "                                np.array([0, 0, 160], dtype=np.float),\n",
    "                                np.array([255, 255, 255], dtype=np.float))\n",
    "    thresh_ground[thresh_rock] = 0 # rocks are not ground\n",
    "    thresh_ground = cv2.morphologyEx(thresh_ground, cv2.MORPH_CLOSE, kernel)\n",
    "    thresh_ground = thresh_ground > 0\n",
    "\n",
    "    thresh_obstacle = 1 - thresh_ground\n",
    "\n",
    "    thresh_img = np.zeros_like(img)\n",
    "\n",
    "    \n",
    "    ground_x, ground_y = thresh_ground.nonzero()\n",
    "    thresh_img[ground_x, ground_y,:] = [0, 100, 0]\n",
    "\n",
    "    obstacle_x, obstacle_y = thresh_obstacle.nonzero()\n",
    "    thresh_img[obstacle_x, obstacle_y,:] = [100, 0, 0]\n",
    "    \n",
    "    rock_x, rock_y = thresh_rock.nonzero()\n",
    "    thresh_img[rock_x, rock_y,:] = [200, 200, 0]\n",
    "\n",
    "    return {\n",
    "        'rock': thresh_rock,\n",
    "        'ground': thresh_ground,\n",
    "        'obstacle': thresh_obstacle,\n",
    "        'img': thresh_img\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    " \n",
    "## Populate the process_image() function with the appropriate analysis steps to map pixels identifying navigable terrain, obstacles and rock samples into a worldmap. Run process_image() on your test data using the moviepy functions provided to create video output of your result.\n",
    "\n",
    " * Describe in your writeup how you modified the process_image() to demonstrate your analysis and how you created a worldmap. Include your video output with your submission.\n",
    "\n",
    "### Answer\n",
    "\n",
    "I compared my calibration image to the test image and didn't find it necessary to modify the source an destination points from what was originally given. I then applied a perspective transform to obtain the warped image. The warped image was sent to the color thresholds to do the segmentation. The pixels corresponding to rock/navigable/and obstacles were then converted into world coordinates. The map at the given world coordinates was then incremented by a fixed amount each time the feature was seen.\n",
    "\n",
    "Test output video [Test Output Video](./output/test_mapping.mp4)\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# Define a function to pass stored images to\n",
    "# reading rover position and yaw angle from csv file\n",
    "# This function will be used by moviepy to create an output video\n",
    "def process_image(img):\n",
    "    # Example of how to use the Databucket() object defined above\n",
    "    # to print the current x, y and yaw values \n",
    "    # print(data.xpos[data.count], data.ypos[data.count], data.yaw[data.count])\n",
    "    xpos = data.xpos[data.count]\n",
    "    ypos = data.ypos[data.count]\n",
    "    yaw = data.yaw[data.count]\n",
    "\n",
    "    # TODO: \n",
    "    # 1) Define source and destination points for perspective transform\n",
    "    \n",
    "\n",
    "    # Define calibration box in source (actual) and destination (desired) coordinates\n",
    "    # These source and destination points are defined to warp the image\n",
    "    # to a grid where each 10x10 pixel square represents 1 square meter\n",
    "    # The destination box will be 2*dst_size on each side\n",
    "    dst_size = 5 \n",
    "    # Set a bottom offset to account for the fact that the bottom of the image \n",
    "    # is not the position of the rover but a bit in front of it\n",
    "    # this is just a rough guess, feel free to change it!\n",
    "    bottom_offset = 6\n",
    "    source = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])\n",
    "    destination = np.float32([[image.shape[1]/2 - dst_size, image.shape[0] - bottom_offset],\n",
    "                      [image.shape[1]/2 + dst_size, image.shape[0] - bottom_offset],\n",
    "                      [image.shape[1]/2 + dst_size, image.shape[0] - 2*dst_size - bottom_offset], \n",
    "                      [image.shape[1]/2 - dst_size, image.shape[0] - 2*dst_size - bottom_offset],\n",
    "                      ])\n",
    "    \n",
    "    # 2) Apply perspective transform\n",
    "    warped = perspect_transform(img, source, destination)\n",
    "\n",
    "    # 3) Apply color threshold to identify navigable terrain/obstacles/rock samples\n",
    "    seg = color_segmenter(warped)\n",
    "    \n",
    "    # 4) Convert thresholded image pixel values to rover-centric coords\n",
    "    navigable_pix_x, navigable_pix_y = rover_coords(seg['ground'])\n",
    "    obstacle_pix_x, obstacle_pix_y = rover_coords(seg['obstacle'])\n",
    "    rock_pix_x, rock_pix_y = rover_coords(seg['rock'])\n",
    "\n",
    "\n",
    "    # 5) Convert rover-centric pixel values to world coords\n",
    "    world_size = 200\n",
    "    scale = 5\n",
    "    obstacle_x_world, obstacle_y_world = pix_to_world(\n",
    "        obstacle_pix_x, obstacle_pix_y,\n",
    "        xpos, ypos, yaw, world_size, scale)\n",
    "    rock_x_world, rock_y_world = pix_to_world(\n",
    "        rock_pix_x, rock_pix_y,\n",
    "        xpos, ypos, yaw, world_size, scale)\n",
    "    navigable_x_world, navigable_y_world = pix_to_world(\n",
    "        navigable_pix_x, navigable_pix_y,\n",
    "        xpos, ypos, yaw, world_size, scale)\n",
    "\n",
    "    # 6) Update worldmap (to be displayed on right side of screen)\n",
    "    data.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n",
    "    data.worldmap[rock_y_world, rock_x_world, 1] += 255\n",
    "    data.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n",
    "\n",
    "    # 7) Make a mosaic image, below is some example code\n",
    "        # First create a blank image (can be whatever shape you like)\n",
    "    output_image = np.zeros((img.shape[0] + data.worldmap.shape[0], img.shape[1]*2, 3))\n",
    "        # Next you can populate regions of the image with various output\n",
    "        # Here I'm putting the original image in the upper left hand corner\n",
    "    output_image[0:img.shape[0], 0:img.shape[1]] = img\n",
    "\n",
    "        # Let's create more images to add to the mosaic, first a warped image\n",
    "    warped = perspect_transform(img, source, destination)\n",
    "        # Add the warped image in the upper right hand corner\n",
    "    output_image[0:img.shape[0], img.shape[1]:] = warped\n",
    "\n",
    "    # Overlay worldmap with ground truth map\n",
    "    map_add = cv2.addWeighted(data.worldmap, 1, data.ground_truth, 0.5, 0)\n",
    "    # Flip map overlay so y-axis points upward and add to output_image \n",
    "    output_image[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)\n",
    "\n",
    "    # output threshhold\n",
    "    output_image[img.shape[0]:2*img.shape[0], img.shape[1]:2*img.shape[1], :] = seg['img']\n",
    "\n",
    "        # Then putting some text over the image\n",
    "    cv2.putText(output_image,\"\", (20, 20), \n",
    "                cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n",
    "    data.count += 1 # Keep track of the index in the Databucket()\n",
    "    \n",
    "\n",
    "    return output_image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomous Navigation and Mapping\n",
    "\n",
    "\n",
    "## Fill in the perception_step() (at the bottom of the perception.py script) and decision_step() (in decision.py) functions in the autonomous mapping scripts and an explanation is provided in the writeup of how and why these functions were modified as they were.\n",
    "\n",
    " * perception_step() and decision_step() functions have been filled in and their functionality explained in the writeup.\n",
    " \n",
    " ### Answer\n",
    " \n",
    " For the perception step I followed the notebook process_image function closely. I also added calculation of the navigable angles and distances as well as a check if the vehicle was level before publishing the maps due to discrepancy in the perpective transform at large roll and pitch angles.\n",
    " \n",
    " \n",
    " ```python\n",
    " # Apply the above functions in succession and update the Rover state accordingly\n",
    "def perception_step(Rover):\n",
    "    # Perform perception steps to update Rover()\n",
    "    # TODO: \n",
    "    # NOTE: camera image is coming to you in Rover.img\n",
    "\n",
    "    img = Rover.img\n",
    "    xpos = Rover.pos[0]\n",
    "    ypos = Rover.pos[1]\n",
    "    yaw = Rover.yaw\n",
    "    roll = Rover.roll\n",
    "    pitch = Rover.pitch\n",
    "\n",
    "    # 1) Define source and destination points for perspective transform\n",
    "\n",
    "    # Define calibration box in source (actual) and destination (desired) coordinates\n",
    "    # These source and destination points are defined to warp the image\n",
    "    # to a grid where each 10x10 pixel square represents 1 square meter\n",
    "    # The destination box will be 2*dst_size on each side\n",
    "    dst_size = 5 \n",
    "\n",
    "    # Set a bottom offset to account for the fact that the bottom of the image \n",
    "    # is not the position of the rover but a bit in front of it\n",
    "    # this is just a rough guess, feel free to change it!\n",
    "    bottom_offset = 6\n",
    "\n",
    "    source = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])\n",
    "    destination = np.float32([[img.shape[1]/2 - dst_size, img.shape[0] - bottom_offset],\n",
    "                      [img.shape[1]/2 + dst_size, img.shape[0] - bottom_offset],\n",
    "                      [img.shape[1]/2 + dst_size, img.shape[0] - 2*dst_size - bottom_offset], \n",
    "                      [img.shape[1]/2 - dst_size, img.shape[0] - 2*dst_size - bottom_offset],\n",
    "                      ])\n",
    "    # 2) Apply perspective transform\n",
    "    warped = perspect_transform(img, source, destination)\n",
    "\n",
    "    # 3) Apply color threshold to identify navigable terrain/obstacles/rock samples\n",
    "    seg = color_segmenter(warped)\n",
    "\n",
    "    # 4) Update Rover.vision_image (this will be displayed on left side of screen)\n",
    "        # Example: Rover.vision_image[:,:,0] = obstacle color-thresholded binary image\n",
    "        #          Rover.vision_image[:,:,1] = rock_sample color-thresholded binary image\n",
    "        #          Rover.vision_image[:,:,2] = navigable terrain color-thresholded binary image\n",
    "    Rover.vision_image = seg['img']\n",
    "\n",
    "    # 5) Convert map image pixel values to rover-centric coords\n",
    "    navigable_pix_x, navigable_pix_y = rover_coords(seg['ground'])\n",
    "    obstacle_pix_x, obstacle_pix_y = rover_coords(seg['obstacle'])\n",
    "    rock_pix_x, rock_pix_y = rover_coords(seg['rock'])\n",
    "\n",
    "    # 6) Convert rover-centric pixel values to world coordinates\n",
    "    world_size = 200\n",
    "    scale = 10\n",
    "    obstacle_x_world, obstacle_y_world = pix_to_world(\n",
    "        obstacle_pix_x, obstacle_pix_y,\n",
    "        xpos, ypos, yaw, world_size, scale)\n",
    "    rock_x_world, rock_y_world = pix_to_world(\n",
    "        rock_pix_x, rock_pix_y,\n",
    "        xpos, ypos, yaw, world_size, scale)\n",
    "\n",
    "    if len(rock_x_world) > 0:\n",
    "        goal_dist, goal_angle = to_polar_coords(np.mean(rock_pix_x), np.mean(rock_pix_y))\n",
    "        Rover.goal_dist = goal_dist\n",
    "        Rover.goal_angle = goal_angle\n",
    "\n",
    "\n",
    "    navigable_x_world, navigable_y_world = pix_to_world(\n",
    "        navigable_pix_x, navigable_pix_y,\n",
    "        xpos, ypos, yaw, world_size, scale)\n",
    "\n",
    "    # 7) Update Rover worldmap (to be displayed on right side of screen)\n",
    "\n",
    "    # only update map if we have small roll/pitch so that the \n",
    "    # perspective transform is valid\n",
    "    if roll > 180:\n",
    "        roll -= 360\n",
    "    if pitch > 180:\n",
    "        pitch -= 360\n",
    "    if Rover.mode != 'pickup' and np.abs(roll) < 1 and np.abs(pitch) < 1:\n",
    "        Rover.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n",
    "        Rover.worldmap[rock_y_world, rock_x_world, 1] += 1\n",
    "        Rover.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n",
    "\n",
    "    # 8) Convert rover-centric pixel positions to polar coordinates\n",
    "    # Update Rover pixel distances and angles\n",
    "        # Rover.nav_dists = rover_centric_pixel_distances\n",
    "        # Rover.nav_angles = rover_centric_angles\n",
    "    dist, angles = to_polar_coords(navigable_pix_x, navigable_pix_y)\n",
    "    Rover.nav_dists = dist\n",
    "    Rover.nav_angles = angles\n",
    "    return Rover\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the decision function I add the following:\n",
    "\n",
    "1. I added a function for automatic pickup if the vehicle is near a sample. I also added a pickup mode so it will wait to finish picking up before trying to continue.\n",
    "2. I implemented a weighted mean for the steering law. The was similar to the example but I wegiht the left steering direction more heavily so it hugs walls more. I also weighted navigable terrain at a greater distance.\n",
    "\n",
    "\n",
    "```python\n",
    "# This is where you can build a decision tree for determining throttle, brake and steer \n",
    "# commands based on the output of the perception_step() function\n",
    "def decision_step(Rover):\n",
    "\n",
    "    # Implement conditionals to decide what to do given perception data\n",
    "    # Here you're all set up with some basic functionality but you'll need to\n",
    "    # improve on this decision tree to do a good job of navigating autonomously!\n",
    "\n",
    "    # pickup rock sample we are near\n",
    "    if Rover.near_sample:\n",
    "        Rover.send_pickup = True\n",
    "        Rover.mode = 'pickup'\n",
    "\n",
    "    # Example:\n",
    "    # Check if we have vision data to make decisions with\n",
    "    if Rover.nav_angles is not None:\n",
    "        # Check for Rover.mode status\n",
    "        if Rover.mode == 'forward': \n",
    "            # Check the extent of navigable terrain\n",
    "            if len(Rover.nav_angles) >= Rover.stop_forward:  \n",
    "                # If mode is forward, navigable terrain looks good \n",
    "                # and velocity is below max, then throttle \n",
    "                if Rover.vel < Rover.max_vel:\n",
    "                    # Set throttle value to throttle setting\n",
    "                    Rover.throttle = Rover.throttle_set\n",
    "                else: # Else coast\n",
    "                    Rover.throttle = 0\n",
    "                    Rover.brake = 0\n",
    "\n",
    "                # Set steering to average angle clipped to the range +/- 15\n",
    "                area = 0\n",
    "                angle_sum = 0\n",
    "                total_sum = 0\n",
    "                scale = 10\n",
    "                right_weight = 0.0\n",
    "                left_weight = 0.1\n",
    "                dist_weight = 0.1\n",
    "\n",
    "                # perform a weighted average over angles\n",
    "                for angle, dist in \\\n",
    "                        zip(Rover.nav_angles, Rover.nav_dists):\n",
    "                    weight = 1\n",
    "                    if angle > 0:\n",
    "                        weight += left_weight\n",
    "                    else:\n",
    "                        weight += right_weight\n",
    "                    # weight further distant points more\n",
    "                    weight += dist_weight*(dist/scale)\n",
    "                    angle_sum += weight*angle\n",
    "                    total_sum += weight\n",
    "                Rover.steer = np.clip(angle_sum/total_sum * 180/np.pi, -15, 15)\n",
    "                # Rover.steer += 1*(Rover.goal_angle - np.deg2rad(Rover.yaw))\n",
    "                # Rover.steer = np.clip(np.mean(Rover.nav_angles * 180/np.pi), -15, 15)\n",
    "\n",
    "            # If there's a lack of navigable terrain pixels then go to 'stop' mode\n",
    "            elif len(Rover.nav_angles) < Rover.stop_forward:\n",
    "                    # Set mode to \"stop\" and hit the brakes!\n",
    "                    Rover.throttle = 0\n",
    "                    # Set brake to stored brake value\n",
    "                    Rover.brake = Rover.brake_set\n",
    "                    Rover.steer = 0\n",
    "                    Rover.mode = 'stop'\n",
    "\n",
    "        # If we're already in \"stop\" mode then make different decisions\n",
    "        elif Rover.mode == 'stop':\n",
    "            # If we're in stop mode but still moving keep braking\n",
    "            if Rover.vel > 0.2:\n",
    "                Rover.throttle = 0\n",
    "                Rover.brake = Rover.brake_set\n",
    "                Rover.steer = 0\n",
    "            # If we're not moving (vel < 0.2) then do something else\n",
    "            elif Rover.vel <= 0.2:\n",
    "                # Now we're stopped and we have vision data to see if there's a path forward\n",
    "                if len(Rover.nav_angles) < Rover.go_forward:\n",
    "                    Rover.throttle = 0\n",
    "                    # Release the brake to allow turning\n",
    "                    Rover.brake = 0\n",
    "                    # Turn range is +/- 15 degrees, when stopped the next line will induce 4-wheel turning\n",
    "                    Rover.steer = -15 # Could be more clever here about which way to turn\n",
    "                # If we're stopped but see sufficient navigable terrain in front then go!\n",
    "                if len(Rover.nav_angles) >= Rover.go_forward:\n",
    "                    # Set throttle back to stored value\n",
    "                    Rover.throttle = Rover.throttle_set\n",
    "                    # Release the brake\n",
    "                    Rover.brake = 0\n",
    "                    # Set steer to mean angle\n",
    "                    Rover.steer = np.clip(np.mean(Rover.nav_angles * 180/np.pi + 5), -15, 15)\n",
    "                    Rover.mode = 'forward'\n",
    "\n",
    "        elif Rover.mode == 'pickup': \n",
    "            # stop and wait until we are finished picking up\n",
    "            Rover.throttle = 0\n",
    "            # Set brake to stored brake value\n",
    "            Rover.brake = Rover.brake_set\n",
    "            Rover.steer = 0\n",
    "            # when we are done picking it up, go back to forward mode\n",
    "            if not Rover.picking_up:\n",
    "                Rover.mode = 'forward'\n",
    "\n",
    "    # Just to make the rover do something \n",
    "    # even if no modifications have been made to the code\n",
    "    else:\n",
    "        Rover.throttle = Rover.throttle_set\n",
    "        Rover.steer = 0\n",
    "        Rover.brake = 0\n",
    "\n",
    "    return Rover\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching in autonomous mode your rover can navigate and map autonomously. Explain your results and how you might improve them in your writeup.\n",
    "\n",
    " * By running drive_rover.pyand launching the simulator in autonomous mode, your rover does a reasonably good job at mapping the environment.\n",
    "\n",
    " * The rover must map at least 40% of the environment with 60% fidelity (accuracy) against the ground truth. You must also find (map) the location of at least one rock sample. They don't need to pick any rocks up, just have them appear in the map (should happen automatically if their map pixels in Rover.worldmap[:,:,1] overlap with sample locations.)\n",
    "\n",
    " * Note: running the simulator with different choices of resolution and graphics quality may produce different results, particularly on different machines! Make a note of your simulator settings (resolution and graphics quality set on launch) and frames per second (FPS output to terminal by drive_rover.py) in your writeup when you submit the project so your reviewer can reproduce your results.\n",
    " \n",
    " ### Answer\n",
    " \n",
    " ![graphics settings](calib_dataset/graphics_settings.png)\n",
    " \n",
    " I was able to get good mapping coverage for most runs, around 80% or better. My fidelity was right around the requirement of 60%.\n",
    " \n",
    "  ![sim_performance](output/sim_performance.png)\n",
    "  \n",
    " Things I would do to improve it if I had time.\n",
    " \n",
    " * Try out VFH obstacle avoidance method instead of mean angle\n",
    " * Implement a get unstuck mode when it doesn't see a rock in the camera but can't move forward.\n",
    " * I wanted to add some sort of boundary closure logic where a goal would be initiated on the dge and progressed forward along the edge until the boundary was complete.\n",
    " * I wanted to have a goal mode, where it wold start tracking the rocks until it reached them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "\n",
    "* Jupyter Notebook with your test code [Jupyter Notebook](./code/Rover_Project_Test_Notebook.ipynb)\n",
    "* Test output video [Test Output Video](./output/test_mapping.mp4)\n",
    "* Autonomous navigation scripts\n",
    "  * [drive_rover.py](code/drive_rover.py)\n",
    "  * [supporting_functions.py](code/supporting_functions.py)\n",
    "  * [decision.py](code/decision.py)\n",
    "  * [perception.py](code/perception.py)\n",
    "* writeup report (md or pdf file)\n",
    "  * [pdf](roboND_jgoppert_p1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Project - Search and Sample Return.ipynb to PDF\n",
      "[NbConvertApp] Writing 21083 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 61001 bytes to roboND_jgoppert_p1.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert Project\\ -\\ Search\\ and\\ Sample\\ Return.ipynb --to PDF --output roboND_jgoppert_p1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
